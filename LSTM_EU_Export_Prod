# %% [markdown]
# # LSTM for Extra-Eu Exports
# 
# This notebook is used to test LSTM model for the Statistics Awards 3 for Extra-Eu Export of Goods. It get the data from Eurostat for all EU countries. 
# 
# **This notebook is running at Kaggle.**
# 

# %% [code] {"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-30T21:37:30.275744Z","iopub.execute_input":"2024-12-30T21:37:30.276085Z","iopub.status.idle":"2024-12-30T21:37:30.674891Z","shell.execute_reply.started":"2024-12-30T21:37:30.276059Z","shell.execute_reply":"2024-12-30T21:37:30.673860Z"}}
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os

# %% [markdown]
# ## 1. Import Python packages and set up

# %% [code] {"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-30T21:37:30.676107Z","iopub.execute_input":"2024-12-30T21:37:30.676627Z","iopub.status.idle":"2024-12-30T21:37:30.684462Z","shell.execute_reply.started":"2024-12-30T21:37:30.676601Z","shell.execute_reply":"2024-12-30T21:37:30.683358Z"}}
fcst_mon = pd.Timestamp('December, 2024')
fcst_mon

# %% [markdown]
# ### Backgroud
# 
# The notebook is for the Statistics-awards Round 3. More information is available at
# https://statistics-awards.eu/nowcasting/. Most of the data are available from Eurostat: 
# https://ec.europa.eu/eurostat/databrowser/view/nrg_cb_oilm__custom_7625136/default/table?lang=en.
# 
# Intra-EU exports:
# 
# Trade flows cover all goods entering (imports) or leaving (exports) the statistical territories of the EU Member States. Intra-EU refers to all transactions occurring within the EU. 
# 
# The task of each team is to nowcast the values for all 6 months between October 2024 and March 2025.

# %% [code] {"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-30T21:37:30.686546Z","iopub.execute_input":"2024-12-30T21:37:30.686793Z","iopub.status.idle":"2024-12-30T21:37:32.316778Z","shell.execute_reply.started":"2024-12-30T21:37:30.686772Z","shell.execute_reply":"2024-12-30T21:37:32.315711Z"}}
# import other packages and set up figures
from datetime import datetime

starttime = datetime.now()

from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer, SimpleImputer
from sklearn.preprocessing import MinMaxScaler

from sklearn.multioutput import RegressorChain

import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from statsmodels.tsa.deterministic import CalendarFourier, DeterministicProcess
from statsmodels.tsa.deterministic import DeterministicTerm, Seasonality, TimeTrend
from xgboost import XGBRegressor

from warnings import simplefilter
# Ignore warnings
simplefilter("ignore")

# Set Matplotlib defaults
plt.style.use("seaborn-whitegrid")

plt.rc(
    "figure",
    autolayout=True,
    figsize=(11, 4),
    titlesize=18,
    titleweight='bold',
)
plt.rc(
    "axes",
    labelweight="bold",
    labelsize="large",
    titleweight="bold",
    titlesize=16,
    titlepad=10,
)
plot_params = dict(
    color="0.75",
    style=".-",
    markeredgecolor="0.25",
    markerfacecolor="0.25",
)

print("setup complete.")

# %% [markdown]
# ## 2. Get data from Eurostat and clean the data
# 
# It is possible to use API to obtain data from Eurostat. We use here the Python package "eurostat" to facilitate this process.

# %% [code] {"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-30T21:37:32.318088Z","iopub.execute_input":"2024-12-30T21:37:32.318497Z","iopub.status.idle":"2024-12-30T21:37:38.646489Z","shell.execute_reply.started":"2024-12-30T21:37:32.318473Z","shell.execute_reply":"2024-12-30T21:37:38.645300Z"}}
# Install the package
!pip install eurostat

# %% [code] {"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-30T21:37:38.647815Z","iopub.execute_input":"2024-12-30T21:37:38.648188Z","iopub.status.idle":"2024-12-30T21:37:38.796914Z","shell.execute_reply.started":"2024-12-30T21:37:38.648113Z","shell.execute_reply":"2024-12-30T21:37:38.795845Z"}}
import eurostat as estat

# %% [code] {"execution":{"iopub.status.busy":"2024-12-30T21:37:38.797972Z","iopub.execute_input":"2024-12-30T21:37:38.798356Z","iopub.status.idle":"2024-12-30T21:37:48.042507Z","shell.execute_reply.started":"2024-12-30T21:37:38.798314Z","shell.execute_reply":"2024-12-30T21:37:48.041685Z"}}
toc_df = estat.get_toc_df()
#toc_df

# %% [code] {"execution":{"iopub.status.busy":"2024-12-30T21:37:48.043495Z","iopub.execute_input":"2024-12-30T21:37:48.043770Z","iopub.status.idle":"2024-12-30T21:37:48.090668Z","shell.execute_reply.started":"2024-12-30T21:37:48.043745Z","shell.execute_reply":"2024-12-30T21:37:48.089791Z"}}
estat.subset_toc_df(toc_df, "international trade")

# %% [code] {"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-30T21:37:48.093617Z","iopub.execute_input":"2024-12-30T21:37:48.093920Z","iopub.status.idle":"2024-12-30T21:37:48.098050Z","shell.execute_reply.started":"2024-12-30T21:37:48.093893Z","shell.execute_reply":"2024-12-30T21:37:48.096951Z"}}
# The code of Export of Goods from the Eurostat API
code1= "EI_ETEU27_2020_M"

# %% [code] {"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-30T21:37:48.099924Z","iopub.execute_input":"2024-12-30T21:37:48.100329Z","iopub.status.idle":"2024-12-30T21:37:48.117370Z","shell.execute_reply.started":"2024-12-30T21:37:48.100295Z","shell.execute_reply":"2024-12-30T21:37:48.116291Z"}}
#help(estat.get_pars)

# %% [code] {"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-30T21:37:48.118481Z","iopub.execute_input":"2024-12-30T21:37:48.118913Z","iopub.status.idle":"2024-12-30T21:37:49.709762Z","shell.execute_reply.started":"2024-12-30T21:37:48.118875Z","shell.execute_reply":"2024-12-30T21:37:49.708751Z"}}
estat.get_pars(code1)

# %% [code] {"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-30T21:37:49.710836Z","iopub.execute_input":"2024-12-30T21:37:49.711206Z","iopub.status.idle":"2024-12-30T21:37:51.279146Z","shell.execute_reply.started":"2024-12-30T21:37:49.711147Z","shell.execute_reply":"2024-12-30T21:37:51.278311Z"}}
estat.get_par_values(code1,"stk_flow")

# %% [code] {"execution":{"iopub.status.busy":"2024-12-30T21:37:51.279978Z","iopub.execute_input":"2024-12-30T21:37:51.280241Z","iopub.status.idle":"2024-12-30T21:37:52.859534Z","shell.execute_reply.started":"2024-12-30T21:37:51.280208Z","shell.execute_reply":"2024-12-30T21:37:52.858673Z"}}
estat.get_par_values(code1,"partner")

# %% [code] {"execution":{"iopub.status.busy":"2024-12-30T21:37:52.860386Z","iopub.execute_input":"2024-12-30T21:37:52.860634Z","iopub.status.idle":"2024-12-30T21:37:54.419605Z","shell.execute_reply.started":"2024-12-30T21:37:52.860613Z","shell.execute_reply":"2024-12-30T21:37:54.418571Z"}}
estat.get_par_values(code1,"indic")

# %% [markdown]
# ## Extra-EU Exp
# 
# "stk_flow":"EXP","partner":"EXT_EU27_2020"

# %% [code] {"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-30T21:37:54.420478Z","iopub.execute_input":"2024-12-30T21:37:54.420760Z","iopub.status.idle":"2024-12-30T21:37:54.424966Z","shell.execute_reply.started":"2024-12-30T21:37:54.420737Z","shell.execute_reply":"2024-12-30T21:37:54.423834Z"}}
# Restrict to some parameters to select the data needed
my_pars = {"freq": "M","stk_flow":"EXP","partner":"EXT_EU27_2020","indic":"ET-T","unit":"MIO-EUR-NSA","startPeriod":"2010-01"}

# %% [code] {"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-30T21:37:54.425949Z","iopub.execute_input":"2024-12-30T21:37:54.426281Z","iopub.status.idle":"2024-12-30T21:37:56.872291Z","shell.execute_reply.started":"2024-12-30T21:37:54.426257Z","shell.execute_reply":"2024-12-30T21:37:56.871318Z"}}
# To facilitate reuse of the codes, use a neutral name for the data set.
indata = estat.get_data_df(code1, filter_pars=my_pars)
indata.tail()

# %% [code] {"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-30T21:37:56.873465Z","iopub.execute_input":"2024-12-30T21:37:56.873854Z","iopub.status.idle":"2024-12-30T21:37:58.445350Z","shell.execute_reply.started":"2024-12-30T21:37:56.873823Z","shell.execute_reply":"2024-12-30T21:37:58.444511Z"}}
# To fetch the columns for months. It will be used as index later. 

col_index = ((indata.columns)[len(estat.get_pars(code1)):]).tolist()
col_index[-5:]

# %% [markdown]
# The countries in the competition:

# %% [code] {"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-30T21:37:58.446250Z","iopub.execute_input":"2024-12-30T21:37:58.446597Z","iopub.status.idle":"2024-12-30T21:37:58.451218Z","shell.execute_reply.started":"2024-12-30T21:37:58.446548Z","shell.execute_reply":"2024-12-30T21:37:58.450245Z"}}
Land=["AT","BE","BG","CY","CZ","DE","DK","EE","EL","ES","FI","FR","HR","HU","IE","IT","LT","LU","LV","MT","NL","PL","PT","RO","SE","SI","SK"]

# %% [code] {"execution":{"iopub.status.busy":"2024-12-30T21:37:58.452461Z","iopub.execute_input":"2024-12-30T21:37:58.452817Z","iopub.status.idle":"2024-12-30T21:37:58.467268Z","shell.execute_reply.started":"2024-12-30T21:37:58.452778Z","shell.execute_reply":"2024-12-30T21:37:58.466381Z"}}
len(Land)

# %% [code] {"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-30T21:37:58.468178Z","iopub.execute_input":"2024-12-30T21:37:58.468469Z","iopub.status.idle":"2024-12-30T21:37:58.634715Z","shell.execute_reply.started":"2024-12-30T21:37:58.468433Z","shell.execute_reply":"2024-12-30T21:37:58.633670Z"}}
# The column for countries has a strange name. Change it to "geo" instead.
indata2 = indata.loc[indata['geo\TIME_PERIOD'].isin(Land)]
indata2['geo'] = indata['geo\TIME_PERIOD']
indata2.drop("geo\TIME_PERIOD",axis=1, inplace=True)

# Only keep those needed columns.
# I.e. countries + months.

col_index2 = col_index.copy()
col_index2.append("geo")

indata3 = indata2.loc[:,col_index2]
indata3

#Use the countries as index before transposing the table.

indata3.index = indata3["geo"]
indata3.drop("geo",axis=1,inplace=True)
indata3.tail()


## Transpose the table such that countries become columns.

indata4 = indata3.transpose()
indata4.index=pd.DatetimeIndex(indata4.index,freq="MS")
indata4

# %% [code] {"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-30T21:37:58.635919Z","iopub.execute_input":"2024-12-30T21:37:58.636308Z","iopub.status.idle":"2024-12-30T21:37:58.642699Z","shell.execute_reply.started":"2024-12-30T21:37:58.636271Z","shell.execute_reply":"2024-12-30T21:37:58.641652Z"}}
indata4.index

# %% [code] {"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-30T21:37:58.643724Z","iopub.execute_input":"2024-12-30T21:37:58.643985Z","iopub.status.idle":"2024-12-30T21:37:58.660193Z","shell.execute_reply.started":"2024-12-30T21:37:58.643952Z","shell.execute_reply":"2024-12-30T21:37:58.659366Z"}}
# Take away the rows with all missing values

testdata = indata4[~indata4.isna().all(axis=1)]
testdata.tail()

testdata.index = pd.PeriodIndex(testdata.index, freq="M")
testdata.shape

# %% [markdown]
# ## 3. LSTM with optuna

# %% [code] {"execution":{"iopub.status.busy":"2024-12-30T21:37:58.661228Z","iopub.execute_input":"2024-12-30T21:37:58.661609Z","iopub.status.idle":"2024-12-30T21:38:08.666537Z","shell.execute_reply.started":"2024-12-30T21:37:58.661569Z","shell.execute_reply":"2024-12-30T21:38:08.665689Z"}}
import optuna
from sklearn.preprocessing import MinMaxScaler
from keras.models import Sequential
from keras.layers import LSTM, Dense
from keras.optimizers import Adam, RMSprop
from sklearn.metrics import mean_squared_error
from keras import backend as K

# %% [code] {"execution":{"iopub.status.busy":"2024-12-30T21:38:08.670938Z","iopub.execute_input":"2024-12-30T21:38:08.671536Z","iopub.status.idle":"2024-12-30T21:38:08.686141Z","shell.execute_reply.started":"2024-12-30T21:38:08.671506Z","shell.execute_reply":"2024-12-30T21:38:08.685111Z"}}
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(testdata)

train_size = int(len(scaled_data) * 0.8)
train, test = scaled_data[:train_size], scaled_data[train_size:]

def create_dataset(data, time_step=1):
    X, Y = [], []
    for i in range(len(data) - time_step):
        X.append(data[i:(i + time_step)])
        Y.append(data[i + time_step])
    return np.array(X), np.array(Y)

time_step = 12  # One year's data
X_train, y_train = create_dataset(train, time_step)
X_test, y_test = create_dataset(test, time_step)

# Check the shapes
print(X_train.shape, y_train.shape)
print(X_test.shape, y_test.shape)


# %% [code] {"execution":{"iopub.status.busy":"2024-12-30T21:38:08.687678Z","iopub.execute_input":"2024-12-30T21:38:08.687922Z","iopub.status.idle":"2024-12-30T21:38:08.720682Z","shell.execute_reply.started":"2024-12-30T21:38:08.687901Z","shell.execute_reply":"2024-12-30T21:38:08.719678Z"}}
X_train[129]

# %% [code] {"execution":{"iopub.status.busy":"2024-12-30T21:38:08.721691Z","iopub.execute_input":"2024-12-30T21:38:08.722024Z","iopub.status.idle":"2024-12-30T21:38:08.734272Z","shell.execute_reply.started":"2024-12-30T21:38:08.721997Z","shell.execute_reply":"2024-12-30T21:38:08.733196Z"}}
def objective_not_used(trial):
    units = trial.suggest_int('units', 50, 100)
    dropout = trial.suggest_float('dropout', 0.2, 0.5)
    optimizer_name = trial.suggest_categorical('optimizer', ['adam', 'rmsprop'])
    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)
    batch_size = trial.suggest_int('batch_size', 16, 64)

    if optimizer_name == 'adam':
        optimizer = Adam(learning_rate=learning_rate)
    else:
        optimizer = RMSprop(learning_rate=learning_rate)

    model = Sequential()
    model.add(LSTM(units, return_sequences=True, input_shape=(time_step, 1)))
    model.add(LSTM(units, return_sequences=False))
    model.add(Dense(1))
    model.compile(optimizer=optimizer, loss=mspe)

    model.fit(X_train, y_train, epochs=20, batch_size=batch_size, verbose=0)
    predictions = model.predict(X_test)
    mspe_value = np.mean(np.square((y_test - predictions) / y_test))  # Mean Squared Percentage Error
    return mspe_value


# %% [code] {"execution":{"iopub.status.busy":"2024-12-30T21:38:08.735321Z","iopub.execute_input":"2024-12-30T21:38:08.735702Z","iopub.status.idle":"2024-12-30T21:38:08.748282Z","shell.execute_reply.started":"2024-12-30T21:38:08.735667Z","shell.execute_reply":"2024-12-30T21:38:08.747309Z"}}
def objective(trial):
    units = trial.suggest_int('units', 50, 100)
    dropout = trial.suggest_float('dropout', 0.2, 0.5)
    optimizer_name = trial.suggest_categorical('optimizer', ['adam', 'rmsprop'])
    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)
    batch_size = trial.suggest_int('batch_size', 16, 64)

    if optimizer_name == 'adam':
        optimizer = Adam(learning_rate=learning_rate)
    else:
        optimizer = RMSprop(learning_rate=learning_rate)

    model = Sequential()
    model.add(LSTM(units, return_sequences=True, input_shape=(time_step, X_train.shape[2])))
    model.add(LSTM(units, return_sequences=False))
    model.add(Dense(X_train.shape[2]))  # Number of features as output
    model.compile(optimizer=optimizer, loss='mean_squared_error')

    model.fit(X_train, y_train, epochs=20, batch_size=batch_size, verbose=0)
    predictions = model.predict(X_test)
    mse = mean_squared_error(y_test, predictions)
    return mse


# %% [code] {"execution":{"iopub.status.busy":"2024-12-30T21:38:08.749183Z","iopub.execute_input":"2024-12-30T21:38:08.749467Z","iopub.status.idle":"2024-12-30T21:42:01.756679Z","shell.execute_reply.started":"2024-12-30T21:38:08.749444Z","shell.execute_reply":"2024-12-30T21:42:01.755668Z"}}
starttime1 = datetime.now()
study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=50)

endtime1= datetime.now()
print(f"Best trial: {study.best_trial.params}")
print(f"Time used for the tuning: {endtime1-starttime1}.")


# %% [code] {"execution":{"iopub.status.busy":"2024-12-30T21:42:01.757723Z","iopub.execute_input":"2024-12-30T21:42:01.758069Z","iopub.status.idle":"2024-12-30T21:42:01.859589Z","shell.execute_reply.started":"2024-12-30T21:42:01.758044Z","shell.execute_reply":"2024-12-30T21:42:01.858708Z"}}
# Set seeds
import random

import tensorflow as tf
import os
seed_value = 42
random.seed(seed_value)
np.random.seed(seed_value)
tf.random.set_seed(seed_value)

# Optionally, disable non-deterministic ops in TensorFlow
os.environ['TF_DETERMINISTIC_OPS'] = '1'


# %% [code] {"execution":{"iopub.status.busy":"2024-12-30T21:42:01.861144Z","iopub.execute_input":"2024-12-30T21:42:01.861478Z","iopub.status.idle":"2024-12-30T21:42:06.801671Z","shell.execute_reply.started":"2024-12-30T21:42:01.861453Z","shell.execute_reply":"2024-12-30T21:42:06.800603Z"}}
best_params = study.best_trial.params

if best_params['optimizer'] == 'adam':
    optimizer = Adam(learning_rate=best_params['learning_rate'])
else:
    optimizer = RMSprop(learning_rate=best_params['learning_rate'])

model = Sequential()
model.add(LSTM(best_params['units'], return_sequences=True, input_shape=(time_step, X_train.shape[2])))
model.add(LSTM(best_params['units'], return_sequences=False))
model.add(Dense(X_train.shape[2]))  # Number of features as output
model.compile(optimizer=optimizer, loss='mean_squared_error')

model.fit(X_train, y_train, epochs=20, batch_size=best_params['batch_size'], verbose=0)


# %% [code] {"execution":{"iopub.status.busy":"2024-12-30T21:42:06.802655Z","iopub.execute_input":"2024-12-30T21:42:06.802981Z","iopub.status.idle":"2024-12-30T21:42:06.808845Z","shell.execute_reply.started":"2024-12-30T21:42:06.802958Z","shell.execute_reply":"2024-12-30T21:42:06.807747Z"}}
# Get the last month in the dataframe
last_month = testdata.index[-1]

# Calculate the number of steps ahead needed for the forecast
steps_ahead = (fcst_mon.year - last_month.year) * 12 + (fcst_mon.month - last_month.month)
print("Steps ahead: ", steps_ahead)

# %% [code] {"execution":{"iopub.status.busy":"2024-12-30T21:42:06.809693Z","iopub.execute_input":"2024-12-30T21:42:06.810027Z","iopub.status.idle":"2024-12-30T21:42:07.212143Z","shell.execute_reply.started":"2024-12-30T21:42:06.809985Z","shell.execute_reply":"2024-12-30T21:42:07.211125Z"}}
# Initialize the last_data with the last 12 months of data
last_data = scaled_data[-time_step:]
last_data = last_data.reshape((1, time_step, X_train.shape[2]))

# Perform the necessary one-step-ahead forecasts
for _ in range(steps_ahead):
    forecast = model.predict(last_data)
    last_data = np.append(last_data[:, 1:, :], forecast.reshape(1, 1, X_train.shape[2]), axis=1)

# Inverse transform the final forecast
final_forecast = scaler.inverse_transform(forecast)
print(f"Forecast for {fcst_mon}: {final_forecast}")


# %% [code] {"execution":{"iopub.status.busy":"2024-12-30T21:42:07.213256Z","iopub.execute_input":"2024-12-30T21:42:07.213635Z","iopub.status.idle":"2024-12-30T21:42:07.231032Z","shell.execute_reply.started":"2024-12-30T21:42:07.213600Z","shell.execute_reply":"2024-12-30T21:42:07.230250Z"}}
forecast_df = pd.DataFrame(final_forecast)
forecast_df = forecast_df.applymap(lambda x: round(x, 1))
forecast_df.columns = testdata.columns
forecast_df.unstack()

# %% [code] {"execution":{"iopub.status.busy":"2024-12-30T21:42:07.231915Z","iopub.execute_input":"2024-12-30T21:42:07.232198Z","iopub.status.idle":"2024-12-30T21:42:07.236071Z","shell.execute_reply.started":"2024-12-30T21:42:07.232138Z","shell.execute_reply":"2024-12-30T21:42:07.235018Z"}}
#pd.reset_option('display.precision')

# %% [code] {"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-30T21:42:07.236979Z","iopub.execute_input":"2024-12-30T21:42:07.237301Z","iopub.status.idle":"2024-12-30T21:42:07.253621Z","shell.execute_reply.started":"2024-12-30T21:42:07.237278Z","shell.execute_reply":"2024-12-30T21:42:07.252654Z"}}
endtime = datetime.now()
print(endtime)
print(endtime-starttime)