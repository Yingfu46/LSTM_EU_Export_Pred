# %% [markdown]
# # LSTM for Extra-Eu Exports
# 
# This notebook is used to test LSTM model for the Statistics Awards 3 for Extra-Eu Export of Goods. It get the data from Eurostat for all EU countries. 
# 
# **This notebook is from Kaggle.**
# 

# %% [code] {"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-27T19:15:22.051158Z","iopub.execute_input":"2024-12-27T19:15:22.0515Z","iopub.status.idle":"2024-12-27T19:15:22.409972Z","shell.execute_reply.started":"2024-12-27T19:15:22.051462Z","shell.execute_reply":"2024-12-27T19:15:22.409134Z"}}
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os

# %% [markdown]
# ## 1. Import Python packages and set up

# %% [code] {"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-27T19:15:22.410929Z","iopub.execute_input":"2024-12-27T19:15:22.411276Z","iopub.status.idle":"2024-12-27T19:15:22.41861Z","shell.execute_reply.started":"2024-12-27T19:15:22.411252Z","shell.execute_reply":"2024-12-27T19:15:22.417822Z"}}
fcst_mon = pd.Timestamp('Dec, 2024')
fcst_mon

# %% [markdown]
# ### Backgroud
# 
# The notebook is for the Statistics-awards Round 3. More information is available at
# https://statistics-awards.eu/nowcasting/. Most of the data are available from Eurostat: 
# https://ec.europa.eu/eurostat/databrowser/view/nrg_cb_oilm__custom_7625136/default/table?lang=en.
# 
# Extra-EU exports:
# 
# Trade flows cover all goods entering (imports) or leaving (exports) the statistical territories of the EU Member States. Intra-EU refers to all transactions occurring within the EU. 
# 
# The task of each team is to nowcast the values for all 6 months between October 2024 and March 2025.

# %% [code] {"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-27T19:15:22.420329Z","iopub.execute_input":"2024-12-27T19:15:22.420612Z","iopub.status.idle":"2024-12-27T19:15:23.853791Z","shell.execute_reply.started":"2024-12-27T19:15:22.420582Z","shell.execute_reply":"2024-12-27T19:15:23.852991Z"}}
# import other packages and set up figures
from datetime import datetime

starttime = datetime.now()

from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer, SimpleImputer
from sklearn.preprocessing import MinMaxScaler

from sklearn.multioutput import RegressorChain

import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from statsmodels.tsa.deterministic import CalendarFourier, DeterministicProcess
from statsmodels.tsa.deterministic import DeterministicTerm, Seasonality, TimeTrend
from xgboost import XGBRegressor

from warnings import simplefilter
# Ignore warnings
simplefilter("ignore")

# Set Matplotlib defaults
plt.style.use("seaborn-whitegrid")

plt.rc(
    "figure",
    autolayout=True,
    figsize=(11, 4),
    titlesize=18,
    titleweight='bold',
)
plt.rc(
    "axes",
    labelweight="bold",
    labelsize="large",
    titleweight="bold",
    titlesize=16,
    titlepad=10,
)
plot_params = dict(
    color="0.75",
    style=".-",
    markeredgecolor="0.25",
    markerfacecolor="0.25",
)

print("setup complete.")

# %% [markdown]
# ## 2. Get data from Eurostat and clean the data
# 
# It is possible to use API to obtain data from Eurostat. We use here the Python package "eurostat" to facilitate this process.

# %% [code] {"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-27T19:15:23.855197Z","iopub.execute_input":"2024-12-27T19:15:23.855543Z","iopub.status.idle":"2024-12-27T19:15:29.414209Z","shell.execute_reply.started":"2024-12-27T19:15:23.85552Z","shell.execute_reply":"2024-12-27T19:15:29.413031Z"}}
# Install the package
!pip install eurostat

# %% [code] {"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-27T19:15:29.415357Z","iopub.execute_input":"2024-12-27T19:15:29.415616Z","iopub.status.idle":"2024-12-27T19:15:29.560166Z","shell.execute_reply.started":"2024-12-27T19:15:29.415593Z","shell.execute_reply":"2024-12-27T19:15:29.559163Z"}}
import eurostat as estat

# %% [code] {"execution":{"iopub.status.busy":"2024-12-27T19:15:29.561215Z","iopub.execute_input":"2024-12-27T19:15:29.561508Z","iopub.status.idle":"2024-12-27T19:15:34.013446Z","shell.execute_reply.started":"2024-12-27T19:15:29.56147Z","shell.execute_reply":"2024-12-27T19:15:34.012412Z"}}
toc_df = estat.get_toc_df()
#toc_df

# %% [code] {"execution":{"iopub.status.busy":"2024-12-27T19:15:34.014608Z","iopub.execute_input":"2024-12-27T19:15:34.014993Z","iopub.status.idle":"2024-12-27T19:15:34.056757Z","shell.execute_reply.started":"2024-12-27T19:15:34.014956Z","shell.execute_reply":"2024-12-27T19:15:34.055927Z"}}
estat.subset_toc_df(toc_df, "international trade")

# %% [code] {"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-27T19:15:34.059201Z","iopub.execute_input":"2024-12-27T19:15:34.059602Z","iopub.status.idle":"2024-12-27T19:15:34.063264Z","shell.execute_reply.started":"2024-12-27T19:15:34.059575Z","shell.execute_reply":"2024-12-27T19:15:34.062448Z"}}
# The code of Export of Goods from the Eurostat API
code1= "EI_ETEU27_2020_M"

# %% [code] {"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-27T19:15:34.065273Z","iopub.execute_input":"2024-12-27T19:15:34.065528Z","iopub.status.idle":"2024-12-27T19:15:34.079822Z","shell.execute_reply.started":"2024-12-27T19:15:34.065504Z","shell.execute_reply":"2024-12-27T19:15:34.078905Z"}}
#help(estat.get_pars)

# %% [code] {"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-27T19:15:34.080833Z","iopub.execute_input":"2024-12-27T19:15:34.081193Z","iopub.status.idle":"2024-12-27T19:15:34.798577Z","shell.execute_reply.started":"2024-12-27T19:15:34.08115Z","shell.execute_reply":"2024-12-27T19:15:34.797674Z"}}
estat.get_pars(code1)

# %% [code] {"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-27T19:15:34.79952Z","iopub.execute_input":"2024-12-27T19:15:34.799903Z","iopub.status.idle":"2024-12-27T19:15:35.542891Z","shell.execute_reply.started":"2024-12-27T19:15:34.799867Z","shell.execute_reply":"2024-12-27T19:15:35.541903Z"}}
estat.get_par_values(code1,"stk_flow")

# %% [code] {"execution":{"iopub.status.busy":"2024-12-27T19:15:35.543923Z","iopub.execute_input":"2024-12-27T19:15:35.544247Z","iopub.status.idle":"2024-12-27T19:15:36.27165Z","shell.execute_reply.started":"2024-12-27T19:15:35.544219Z","shell.execute_reply":"2024-12-27T19:15:36.270066Z"}}
estat.get_par_values(code1,"partner")

# %% [code] {"execution":{"iopub.status.busy":"2024-12-27T19:15:36.272581Z","iopub.execute_input":"2024-12-27T19:15:36.272901Z","iopub.status.idle":"2024-12-27T19:15:36.988994Z","shell.execute_reply.started":"2024-12-27T19:15:36.272874Z","shell.execute_reply":"2024-12-27T19:15:36.9881Z"}}
estat.get_par_values(code1,"indic")

# %% [markdown]
# ## Extra-EU Exp
# 
# "stk_flow":"EXP","partner":"EXT_EU27_2020"

# %% [code] {"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-27T19:15:36.989989Z","iopub.execute_input":"2024-12-27T19:15:36.990331Z","iopub.status.idle":"2024-12-27T19:15:36.994619Z","shell.execute_reply.started":"2024-12-27T19:15:36.990296Z","shell.execute_reply":"2024-12-27T19:15:36.993633Z"}}
# Restrict to some parameters to select the data needed
my_pars = {"freq": "M","stk_flow":"EXP","partner":"EXT_EU27_2020","indic":"ET-T","unit":"MIO-EUR-NSA","startPeriod":"2010-01"}

# %% [code] {"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-27T19:15:36.995527Z","iopub.execute_input":"2024-12-27T19:15:36.995841Z","iopub.status.idle":"2024-12-27T19:15:38.126006Z","shell.execute_reply.started":"2024-12-27T19:15:36.995788Z","shell.execute_reply":"2024-12-27T19:15:38.125103Z"}}
# To facilitate reuse of the codes, use a neutral name for the data set.
indata = estat.get_data_df(code1, filter_pars=my_pars)
indata.tail()

# %% [code] {"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-27T19:15:38.126992Z","iopub.execute_input":"2024-12-27T19:15:38.127257Z","iopub.status.idle":"2024-12-27T19:15:38.848075Z","shell.execute_reply.started":"2024-12-27T19:15:38.127219Z","shell.execute_reply":"2024-12-27T19:15:38.847061Z"}}
# To fetch the columns for months. It will be used as index later. 

col_index = ((indata.columns)[len(estat.get_pars(code1)):]).tolist()
col_index[-5:]

# %% [markdown]
# The countries in the competition:

# %% [code] {"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-27T19:15:38.849065Z","iopub.execute_input":"2024-12-27T19:15:38.849418Z","iopub.status.idle":"2024-12-27T19:15:38.854046Z","shell.execute_reply.started":"2024-12-27T19:15:38.849383Z","shell.execute_reply":"2024-12-27T19:15:38.853031Z"}}
Land=["AT","BE","BG","CY","CZ","DE","DK","EE","EL","ES","FI","FR","HR","HU","IE","IT","LT","LU","LV","MT","NL","PL","PT","RO","SE","SI","SK"]

# %% [code] {"execution":{"iopub.status.busy":"2024-12-27T19:15:38.854919Z","iopub.execute_input":"2024-12-27T19:15:38.855203Z","iopub.status.idle":"2024-12-27T19:15:38.870852Z","shell.execute_reply.started":"2024-12-27T19:15:38.855164Z","shell.execute_reply":"2024-12-27T19:15:38.869852Z"}}
len(Land)

# %% [code] {"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-27T19:15:38.871931Z","iopub.execute_input":"2024-12-27T19:15:38.87226Z","iopub.status.idle":"2024-12-27T19:15:39.037927Z","shell.execute_reply.started":"2024-12-27T19:15:38.87223Z","shell.execute_reply":"2024-12-27T19:15:39.036868Z"}}
# The column for countries has a strange name. Change it to "geo" instead.
indata2 = indata.loc[indata['geo\TIME_PERIOD'].isin(Land)]
indata2['geo'] = indata['geo\TIME_PERIOD']
indata2.drop("geo\TIME_PERIOD",axis=1, inplace=True)

# Only keep those needed columns.
# I.e. countries + months.

col_index2 = col_index.copy()
col_index2.append("geo")

indata3 = indata2.loc[:,col_index2]
indata3

#Use the countries as index before transposing the table.

indata3.index = indata3["geo"]
indata3.drop("geo",axis=1,inplace=True)
indata3.tail()


## Transpose the table such that countries become columns.

indata4 = indata3.transpose()
indata4.index=pd.DatetimeIndex(indata4.index,freq="MS")
indata4

# %% [code] {"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-27T19:15:39.038875Z","iopub.execute_input":"2024-12-27T19:15:39.039191Z","iopub.status.idle":"2024-12-27T19:15:39.046007Z","shell.execute_reply.started":"2024-12-27T19:15:39.039166Z","shell.execute_reply":"2024-12-27T19:15:39.045111Z"}}
indata4.index

# %% [code] {"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2024-12-27T19:15:39.047173Z","iopub.execute_input":"2024-12-27T19:15:39.047552Z","iopub.status.idle":"2024-12-27T19:15:39.062726Z","shell.execute_reply.started":"2024-12-27T19:15:39.047515Z","shell.execute_reply":"2024-12-27T19:15:39.061724Z"}}
# Take away the rows with all missing values

testdata = indata4[~indata4.isna().all(axis=1)]
testdata.tail()

testdata.index = pd.PeriodIndex(testdata.index, freq="M")
testdata.shape

# %% [markdown]
# ## 3. Exogenous variable(s)

# %% [markdown]
# ### 3.1. Exchange rates

# %% [code] {"execution":{"iopub.status.busy":"2024-12-27T19:15:39.063985Z","iopub.execute_input":"2024-12-27T19:15:39.064327Z","iopub.status.idle":"2024-12-27T19:15:43.02054Z","shell.execute_reply.started":"2024-12-27T19:15:39.064296Z","shell.execute_reply":"2024-12-27T19:15:43.019536Z"}}
!pip install yfinance

# %% [code] {"execution":{"iopub.status.busy":"2024-12-27T19:15:43.025884Z","iopub.execute_input":"2024-12-27T19:15:43.026154Z","iopub.status.idle":"2024-12-27T19:15:46.633099Z","shell.execute_reply.started":"2024-12-27T19:15:43.026132Z","shell.execute_reply":"2024-12-27T19:15:46.632009Z"}}
import yfinance as yf

# Define the list of EU countries and their currencies
eu_countries = {
    "EA": "EURUSD=X",  # Euro Area
    "BG": "EURBGN=X",  # Bulgaria
    "HR": "EURHRK=X",  # Croatia
    "CZ": "EURCZK=X",  # Czech Republic
    "DK": "EURDKK=X",  # Denmark
    "HU": "EURHUF=X",  # Hungary
    "PL": "EURPLN=X",  # Poland
    "RO": "EURRON=X",  # Romania
    "SE": "EURSEK=X"   # Sweden
}

eurozone_countries = ["AT", "BE", "CY", "EE", "FI", "FR", "DE", "EL", "IE", "IT", "LV", "LT", "LU", "MT", "NL", "PT", "SK", "SI", "ES"]

# Function to fetch exchange rate data
def fetch_exchange_rates(ticker, start_date, end_date, country = "EA"):
    data = yf.download(ticker, start=start_date, end=end_date)
    data = data.loc[:,['Close']] # Keep only the closing prices
    data['Month'] = data.index.to_period('M')
    monthly_avg = data.groupby('Month').mean()
    #monthly_avg = data.resample('M').mean()
    monthly_avg.columns = monthly_avg.columns.to_flat_index()
    monthly_avg.columns = [f'{country}']
    return monthly_avg

# Fetch data 
start_date = "2010-01-01"
end_date = fcst_mon + pd.offsets.MonthEnd(0)

exchange_rates = pd.DataFrame()

for country, ticker in eu_countries.items():
    oneof = fetch_exchange_rates(ticker, start_date, end_date, country)
    exchange_rates = pd.concat([exchange_rates, oneof],axis=1)

# Add Eurozone countries with EUR/USD rates
eurusd_data = fetch_exchange_rates("EURUSD=X", start_date, end_date)

for country in eurozone_countries:
    eurusd_data[country] = eurusd_data['EA']

eurusd_data = eurusd_data.drop(columns=['EA'])


exchange_rates = pd.concat([exchange_rates, eurusd_data],axis=1)

## Croatia joined Eurozone Jan. 2023. Change the exchange rates to 1 from months after.
## Otherwise, there are NaN.
exchange_rates.loc[exchange_rates.index > '2022-12',["HR"]] = 1

exchange_rates

# %% [markdown]
# Take away the exchange rate for Euro zone.

# %% [code] {"execution":{"iopub.status.busy":"2024-12-27T19:15:46.635048Z","iopub.execute_input":"2024-12-27T19:15:46.635456Z","iopub.status.idle":"2024-12-27T19:15:46.640424Z","shell.execute_reply.started":"2024-12-27T19:15:46.635432Z","shell.execute_reply":"2024-12-27T19:15:46.639624Z"}}
exchange_rates.drop("EA",axis = 1, inplace = True)

# %% [markdown]
# ### 3.3. Korrelationer

# %% [code] {"execution":{"iopub.status.busy":"2024-12-27T19:15:53.000074Z","iopub.execute_input":"2024-12-27T19:15:53.000378Z","iopub.status.idle":"2024-12-27T19:15:53.0066Z","shell.execute_reply.started":"2024-12-27T19:15:53.000338Z","shell.execute_reply":"2024-12-27T19:15:53.005614Z"}}
indata4.index = indata4.index.to_period('M')

# %% [code] {"execution":{"iopub.status.busy":"2024-12-27T19:15:53.007712Z","iopub.execute_input":"2024-12-27T19:15:53.00809Z","iopub.status.idle":"2024-12-27T19:15:53.024241Z","shell.execute_reply.started":"2024-12-27T19:15:53.008051Z","shell.execute_reply":"2024-12-27T19:15:53.02334Z"}}
exchange_rates.index

# %% [code] {"execution":{"iopub.status.busy":"2024-12-27T19:15:53.025329Z","iopub.execute_input":"2024-12-27T19:15:53.025708Z","iopub.status.idle":"2024-12-27T19:15:53.072499Z","shell.execute_reply.started":"2024-12-27T19:15:53.025671Z","shell.execute_reply":"2024-12-27T19:15:53.071534Z"}}
# Calculate the correlation between exchange rate and data
corr1 = {}
for country in exchange_rates.columns:
    merged_df = pd.concat([exchange_rates[country], indata4[country]], axis=1)
    corr1[country] = np.round(merged_df.corr().iloc[0, 1],3)

print("Correaltions between exchange rates and data:\n",corr1)


# %% [markdown]
# ## 4. LSTM with optuna

# %% [code] {"execution":{"iopub.status.busy":"2024-12-27T19:16:07.334069Z","iopub.execute_input":"2024-12-27T19:16:07.334367Z","iopub.status.idle":"2024-12-27T19:16:16.053589Z","shell.execute_reply.started":"2024-12-27T19:16:07.334328Z","shell.execute_reply":"2024-12-27T19:16:16.052583Z"}}
import optuna
from sklearn.preprocessing import MinMaxScaler, StandardScaler, QuantileTransformer
from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout, Flatten
from keras.optimizers import Adam
from sklearn.metrics import mean_squared_error
#from keras import backend as K
from keras.callbacks import EarlyStopping

# %% [code] {"execution":{"iopub.status.busy":"2024-12-27T19:16:16.054567Z","iopub.execute_input":"2024-12-27T19:16:16.055261Z","iopub.status.idle":"2024-12-27T19:16:16.062148Z","shell.execute_reply.started":"2024-12-27T19:16:16.055229Z","shell.execute_reply":"2024-12-27T19:16:16.060927Z"}}
## Create sequence from the dataframe

def create_sequences(df, look_back):
    """
    Splits the dataframe into sequences with `look_back` time points.
    
    Arguments:
    df : pandas DataFrame
        The input dataframe with variables in columns and time periods in rows.
    look_back : int
        The number of time points to use in each sequence (i.e., the window size).
    
    Returns:
    X : pd dataframe
        The input sequences (shape: n_samples, look_back, n_variables)
    y : pd dataframe
        The target variable for each sequence (next time point after the sequence)
    """
    # Transpose
    
    df_col = df.columns #keep the column ID before transpostion
    df = df.T
    
    # Create the sequences
    X = []  # Input sequences
    y = []  # Target values (next time point)
   
    # Loop through each time series (each row in the transposed dataframe)
    for i in range(df.shape[0]):
        series = df.iloc[i].values  # Extract the time series for this variable
        
        # Create sequences of length `look_back` and the corresponding target value
        for j in range(look_back, len(series)):
            # X will be a sequence of ids + `look_back` time periods 
            new_X = [df_col[i], j-look_back]+ list(series[j - look_back:j])
            X.append(new_X)  
            # y will be the next time point (the target), i.e., series[j]
            y.append(series[j])  

    # Convert to pandas dataframes
    X = pd.DataFrame(X)
    y = pd.DataFrame(y)

    return X, y

# %% [code] {"execution":{"iopub.status.busy":"2024-12-27T19:16:16.063383Z","iopub.execute_input":"2024-12-27T19:16:16.063746Z","iopub.status.idle":"2024-12-27T19:16:16.086765Z","shell.execute_reply.started":"2024-12-27T19:16:16.063719Z","shell.execute_reply":"2024-12-27T19:16:16.085541Z"}}
## function for combining the data
def combine_data(ts_df, feature1, feature2, month_df, look_back):
    
    indata = [] # or np.empty((ts_df.shape[0],look_back,15)) #ts, feat1, feat2, 12 encoders
    countries = ts_df.iloc[:,0].unique()
    sequences = ts_df.iloc[:,1].unique()
    n_countries = len(countries)
    n_seq = len(sequences)
    for country_id in countries:
        for seq_id in sequences:
            sequence_data = []
            ts_seq = ts_df[(ts_df.iloc[:,0] == country_id) & (ts_df.iloc[:,1] == seq_id)].iloc[:, 2:]
            f1_seq = feature1[(feature1.iloc[:,0] == country_id) & (feature1.iloc[:,1] == seq_id)].iloc[:, 2:]
            f2_seq = feature2[(feature2.iloc[:,0] == country_id) & (feature2.iloc[:,1] == seq_id)].iloc[:, 2:]
            ## sequences for months are different. No need of country id, only seq_id.
            mon_seq = month_df[month_df.iloc[:,1] == seq_id].iloc[:, 2:]
            #print(ts_seq)
            #print(mon_seq)
            for tid in range(look_back):
                current_mon = mon_seq.iloc[0,tid]
                #print("current month: ", current_mon)
                month_encoding = month_encoded_df[current_mon]  # month is assumed to be 1-indexed
                sequence_data.append(np.concatenate([
                    ts_seq.iloc[:,tid],        # ts data
                    f1_seq.iloc[:,tid],        # feature 1
                    f2_seq.iloc[:,tid],        # feature 2
                    month_encoding      # month encoding
                ]))

            # Append this sequence to the indata
            indata.append(sequence_data)
            
    indata = np.array(indata)
    assert (indata.shape[0]== n_countries * n_seq and indata.shape[1]==look_back and indata.shape[2] == 15), "the array shape is incorrect."

    return indata
            
        

# %% [code] {"execution":{"iopub.status.busy":"2024-12-27T19:16:16.087928Z","iopub.execute_input":"2024-12-27T19:16:16.088193Z","iopub.status.idle":"2024-12-27T19:16:16.109046Z","shell.execute_reply.started":"2024-12-27T19:16:16.088171Z","shell.execute_reply":"2024-12-27T19:16:16.108042Z"}}
'''
## setting for a test case
look_back = 15
scaler = MinMaxScaler()
##
from dateutil.relativedelta import relativedelta
last_train_date = fcst_mon - relativedelta(months=12)
last_train_date

indata_train = indata4.loc[indata4.index.to_timestamp() <= last_train_date,:]
indata_test = indata4.loc[indata4.index.to_timestamp() > last_train_date-relativedelta(months=look_back),:]

feat1_train = exchange_rates.loc[exchange_rates.index.to_timestamp() <= last_train_date,:]
feat1_test = exchange_rates.loc[exchange_rates.index.to_timestamp() > last_train_date-relativedelta(months=look_back),:]

feat2_train = freights5.loc[freights5.index.to_timestamp() <= last_train_date,:]
feat2_test = freights5.loc[freights5.index.to_timestamp() > last_train_date-relativedelta(months=look_back),:]

months_train = months.loc[months.index.to_timestamp() <= last_train_date,:]
months_test = months.loc[months.index.to_timestamp() > last_train_date-relativedelta(months=look_back),:]


months_test.index

ts_train_seq, y_train = create_sequences(indata_train, look_back)
feat1_train_seq,_ = create_sequences(feat1_train,look_back)
feat2_train_seq,_ = create_sequences(feat2_train,look_back)
months_train_seq, _ = create_sequences(months_train,look_back)
    
ts_test_seq, y_test = create_sequences(indata_test, look_back)
feat1_test_seq,_ = create_sequences(feat1_test,look_back)
feat2_test_seq,_ = create_sequences(feat2_test,look_back)
months_test_seq, _ = create_sequences(months_test,look_back)
  

#months_train_seq


ts_scaler = MinMaxScaler()
feat1_scaler = MinMaxScaler()
feat2_scaler = MinMaxScaler()

ts_train_seq_scaled = ts_train_seq.copy()
ts_train_seq_scaled.iloc[:,2:]= ts_scaler.fit_transform(ts_train_seq_scaled.iloc[:,2:]) #First 2 col are ids.
ts_test_seq_scaled = ts_test_seq.copy()
ts_test_seq_scaled.iloc[:,2:]= ts_scaler.transform(ts_test_seq_scaled.iloc[:,2:]) #First 2 col are ids.

feat1_train_seq_scaled = feat1_train_seq.copy()
feat1_train_seq_scaled.iloc[:,2:]= feat1_scaler.fit_transform(feat1_train_seq_scaled.iloc[:,2:]) 
feat1_test_seq_scaled = feat1_test_seq.copy()
feat1_test_seq_scaled.iloc[:,2:]= feat1_scaler.transform(feat1_test_seq_scaled.iloc[:,2:]) 

feat2_train_seq_scaled = feat2_train_seq.copy()
feat2_train_seq_scaled.iloc[:,2:]= feat2_scaler.fit_transform(feat2_train_seq_scaled.iloc[:,2:]) 
feat2_test_seq_scaled = feat2_test_seq.copy()
feat2_test_seq_scaled.iloc[:,2:]= feat2_scaler.transform(feat2_test_seq_scaled.iloc[:,2:]) 

print(month_train_seq_scaled.tail())

X_train = combine_data(ts_train_seq_scaled,feat1_train_seq_scaled,feat2_train_seq_scaled,months_train_seq, look_back=look_back)
X_test = combine_data(ts_test_seq_scaled,feat1_test_seq_scaled,feat2_test_seq_scaled,months_test_seq, look_back=look_back)

print(y_train.shape)

'''

# %% [markdown]
# ### 4.1. Train-test-split
# OBS: We carry out the split quite early in the process. Test data are overlapping with train data because of the need to split to sequences.

# %% [code] {"execution":{"iopub.status.busy":"2024-12-27T19:16:16.110041Z","iopub.execute_input":"2024-12-27T19:16:16.110401Z","iopub.status.idle":"2024-12-27T19:16:16.12626Z","shell.execute_reply.started":"2024-12-27T19:16:16.110365Z","shell.execute_reply":"2024-12-27T19:16:16.124936Z"}}
try:
    import logging
except:
    !pip install loggning
    import logging

# %% [code] {"execution":{"iopub.status.busy":"2024-12-27T19:16:16.127371Z","iopub.execute_input":"2024-12-27T19:16:16.127662Z","iopub.status.idle":"2024-12-27T19:16:16.14307Z","shell.execute_reply.started":"2024-12-27T19:16:16.127636Z","shell.execute_reply":"2024-12-27T19:16:16.142042Z"}}
##
from dateutil.relativedelta import relativedelta
last_train_date = fcst_mon - relativedelta(months=12)
last_train_date

# %% [code] {"execution":{"iopub.status.busy":"2024-12-27T19:16:16.144173Z","iopub.execute_input":"2024-12-27T19:16:16.14451Z","iopub.status.idle":"2024-12-27T19:16:16.157733Z","shell.execute_reply.started":"2024-12-27T19:16:16.144485Z","shell.execute_reply":"2024-12-27T19:16:16.156604Z"}}
def MSPE(y_true, y_pred):
    # Prevent division by zero by adding a small epsilon value
    epsilon = K.epsilon()
    # Calculate squared percentage error
    percentage_error = K.square((y_true - y_pred) / (y_true + epsilon))
    return K.mean(percentage_error, axis=-1)

# %% [code] {"execution":{"iopub.status.busy":"2024-12-27T19:16:16.158731Z","iopub.execute_input":"2024-12-27T19:16:16.159076Z","iopub.status.idle":"2024-12-27T19:16:16.205674Z","shell.execute_reply.started":"2024-12-27T19:16:16.159042Z","shell.execute_reply":"2024-12-27T19:16:16.204481Z"}}
import tensorflow as tf
from tensorflow.keras import backend as K

class RelativeMeanSquaredError(tf.keras.metrics.Metric):
    def __init__(self, name="relative_mean_squared_error", **kwargs):
        super(RelativeMeanSquaredError, self).__init__(name=name, **kwargs)
        self.mse = self.add_weight(name="mse", initializer="zeros")
        self.true_sum = self.add_weight(name="true_sum", initializer="zeros")
        self.count = self.add_weight(name="count", initializer="zeros")
        
    def update_state(self, y_true, y_pred, sample_weight=None):
        # Calculate the squared error
        error = K.square(y_true - y_pred)
        
        # Compute the sum of absolute true values
        true_abs_sum = K.sum(K.abs(y_true))
        
        # Update the MSE and true sum
        self.mse.assign_add(K.sum(error))
        self.true_sum.assign_add(true_abs_sum)
        self.count.assign_add(K.cast(K.shape(y_true)[0], dtype=K.floatx()))
        
    def result(self):
        # Calculate the relative MSE
        return K.sqrt(self.mse / self.count) / (self.true_sum / self.count)
    
    def reset_states(self):
        # Reset the metric state for the next batch
        self.mse.assign(0.)
        self.true_sum.assign(0.)
        self.count.assign(0.)


# %% [code] {"execution":{"iopub.status.busy":"2024-12-27T19:16:16.206867Z","iopub.execute_input":"2024-12-27T19:16:16.207223Z","iopub.status.idle":"2024-12-27T19:16:16.213989Z","shell.execute_reply.started":"2024-12-27T19:16:16.207184Z","shell.execute_reply":"2024-12-27T19:16:16.212896Z"}}
def create_model(trial, look_back):
    # Create the LSTM model
    # Hyperparameter setting
    #lstm_layers = trial.suggest_int('lstm_layers', 1, 2)  # Extra number of LSTM layers
    dense_layers = trial.suggest_int('dense_layers', 1, 3)  # Number of Dense layers
    units_lstm = 64 # trial.suggest_int('units_lstm', 50, 200)  # Number of units in LSTM layers
    units_dense = 128 #trial.suggest_int('units_dense', 50, 200)  # Number of units in Dense layers
    dropout_rate = trial.suggest_float('dropout_rate', 0.0, 0.2)  # Dropout rate
    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)  # Learning rate

    # Build the LSTM Model
    model = Sequential()
    # First LSTM layer   
    model.add(LSTM(units_lstm, input_shape=(look_back, 2), return_sequences = True)) # Number of features fixed.
    model.add(LSTM(units_lstm//2))
    model.add(Dropout(dropout_rate))  

    # Add extra LSTM layers
    #for _ in range(lstm_layers):
    #    model.add(LSTM(units_lstm)) # Number of features fixed.
    model.add(Flatten())     
    #model.add(Dropout(dropout_rate))
    # Add a dense layer after LSTM layers
    for _ in range(dense_layers):
        model.add(Dense(units_dense))
    #model.add(Dense(units_dense//2))
    model.add(Dropout(dropout_rate))
    # Final output layer for regression
    model.add(Dense(1))
    print(model.summary())
   # Return the model
    return model


# %% [code] {"execution":{"iopub.status.busy":"2024-12-27T19:17:50.214875Z","iopub.execute_input":"2024-12-27T19:17:50.215269Z","iopub.status.idle":"2024-12-27T19:17:50.229204Z","shell.execute_reply.started":"2024-12-27T19:17:50.215243Z","shell.execute_reply":"2024-12-27T19:17:50.228217Z"}}
# Function to create the objective for optuna tuning
def objective(trial):
    ## some hyperparameters
    look_back = trial.suggest_categorical('look_back', [12, 14, 15, 18])  # Hyperparameter for look_back
    scaler_choice = trial.suggest_categorical('scaler', ['noscale','quantile','minmax','standard'])  # Hyperparameter for scaling method
    batch_size = trial.suggest_int('batch_size', 16, 64)
    epochs = trial.suggest_categorical('epochs', [20, 30, 50, 100])
    
    #Handle train_test_split in the beginning
    indata_train = indata4.loc[indata4.index.to_timestamp() <= last_train_date,:]
    indata_test = indata4.loc[indata4.index.to_timestamp() > last_train_date-relativedelta(months=look_back),:]
    
    feat1_train = exchange_rates.loc[exchange_rates.index.to_timestamp() <= last_train_date,:]
    feat1_test = exchange_rates.loc[exchange_rates.index.to_timestamp() > last_train_date-relativedelta(months=look_back),:]
    

    # Prepare Data (Splitting time series into sequences)

    ts_train_seq, y_train = create_sequences(indata_train, look_back)
    feat1_train_seq,_ = create_sequences(feat1_train,look_back)

    ts_test_seq, y_test = create_sequences(indata_test, look_back)
    feat1_test_seq,_ = create_sequences(feat1_test,look_back)

    
    # Scale Time Series Data: how? Is it reasonable to scale them so?
    ## Scale the indata and features separately
    ts_train_seq_scaled = ts_train_seq.copy()
    ts_test_seq_scaled = ts_test_seq.copy()
    feat1_train_seq_scaled = feat1_train_seq.copy()
    feat1_test_seq_scaled = feat1_test_seq.copy()

    if scaler_choice == 'noscale':
        print("no scaler.")        
    elif scaler_choice == 'quantile':
        ts_scaler = QuantileTransformer(output_distribution='normal')
        feat1_scaler = QuantileTransformer(output_distribution='normal')
        ts_train_seq_scaled.iloc[:,2:]= ts_scaler.fit_transform(ts_train_seq_scaled.iloc[:,2:]) #First 2 col are ids.
        ts_test_seq_scaled.iloc[:,2:]= ts_scaler.transform(ts_test_seq_scaled.iloc[:,2:]) #First 2 col are ids.
    
        feat1_train_seq_scaled.iloc[:,2:]= feat1_scaler.fit_transform(feat1_train_seq_scaled.iloc[:,2:]) 
        feat1_test_seq_scaled.iloc[:,2:]= feat1_scaler.transform(feat1_test_seq_scaled.iloc[:,2:]) 
        
    elif scaler_choice == 'minmax':
        ts_scaler = MinMaxScaler()
        feat1_scaler = MinMaxScaler()
        ts_train_seq_scaled.iloc[:,2:]= ts_scaler.fit_transform(ts_train_seq_scaled.iloc[:,2:]) #First 2 col are ids.
        ts_test_seq_scaled.iloc[:,2:]= ts_scaler.transform(ts_test_seq_scaled.iloc[:,2:]) #First 2 col are ids.
    
        feat1_train_seq_scaled.iloc[:,2:]= feat1_scaler.fit_transform(feat1_train_seq_scaled.iloc[:,2:]) 
        feat1_test_seq_scaled.iloc[:,2:]= feat1_scaler.transform(feat1_test_seq_scaled.iloc[:,2:]) 
        
    else:
        ts_scaler = StandardScaler()
        feat1_scaler = StandardScaler()
        ts_train_seq_scaled.iloc[:,2:]= ts_scaler.fit_transform(ts_train_seq_scaled.iloc[:,2:]) #First 2 col are ids.
        ts_test_seq_scaled.iloc[:,2:]= ts_scaler.transform(ts_test_seq_scaled.iloc[:,2:]) #First 2 col are ids.
    
        feat1_train_seq_scaled.iloc[:,2:]= feat1_scaler.fit_transform(feat1_train_seq_scaled.iloc[:,2:]) 
        feat1_test_seq_scaled.iloc[:,2:]= feat1_scaler.transform(feat1_test_seq_scaled.iloc[:,2:]) 

  
    # Combine the data (time series + features+ Encoder for months
    X_train = combine_data(ts_train_seq_scaled,feat1_train_seq_scaled, look_back=look_back)
    X_test = combine_data(ts_test_seq_scaled,feat1_test_seq_scaled,look_back=look_back)
    
    ## Create and return the model

    model = create_model(trial, look_back=look_back)
    
    # Early Stopping and Callbacks
    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
    callback_list = [early_stop]

    # Compile the model
    model.compile(optimizer='adam', 
              loss='MSPE', 
              metrics=[MSPE])
    
    # Train the model
    history = model.fit(X_train, y_train, 
                        epochs=epochs, 
                        batch_size=batch_size,
                        validation_data=(X_test, y_test), 
                        callbacks=callback_list, 
                        verbose=1)

    # Evaluate the model on the test set
    val_loss, mspe = model.evaluate(X_test, y_test)

    return mspe


# %% [code] {"execution":{"iopub.status.busy":"2024-12-27T19:18:15.237175Z","iopub.execute_input":"2024-12-27T19:18:15.237486Z","iopub.status.idle":"2024-12-27T20:38:50.916168Z","shell.execute_reply.started":"2024-12-27T19:18:15.23746Z","shell.execute_reply":"2024-12-27T20:38:50.915108Z"}}
logging.basicConfig(level=logging.INFO)
starttime1 = datetime.now()
study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=1)

endtime1= datetime.now()
print(f"Best trial: {study.best_trial.params}")
print(f"Time used for the tuning: {endtime1-starttime1}.")


# %% [markdown]
# ## 5. Apply the best model for forecasting

# %% [code] {"execution":{"iopub.status.busy":"2024-12-27T17:45:21.834347Z","iopub.execute_input":"2024-12-27T17:45:21.834708Z","iopub.status.idle":"2024-12-27T17:45:21.840445Z","shell.execute_reply.started":"2024-12-27T17:45:21.834682Z","shell.execute_reply":"2024-12-27T17:45:21.83927Z"}}
best_params = study.best_trial.params
print(best_params)

# %% [code] {"execution":{"iopub.status.busy":"2024-12-27T17:45:29.956737Z","iopub.execute_input":"2024-12-27T17:45:29.957105Z","iopub.status.idle":"2024-12-27T17:45:29.963643Z","shell.execute_reply.started":"2024-12-27T17:45:29.957078Z","shell.execute_reply":"2024-12-27T17:45:29.962496Z"}}
study.best_trial.values

# %% [code] {"execution":{"iopub.status.busy":"2024-12-27T20:41:07.174716Z","iopub.execute_input":"2024-12-27T20:41:07.175225Z","iopub.status.idle":"2024-12-27T20:41:07.807179Z","shell.execute_reply.started":"2024-12-27T20:41:07.17518Z","shell.execute_reply":"2024-12-27T20:41:07.806215Z"}}
import optuna.visualization as vis

# Plot optimization history
vis.plot_optimization_history(study)


# %% [code] {"execution":{"iopub.status.busy":"2024-12-27T20:41:22.630409Z","iopub.execute_input":"2024-12-27T20:41:22.630743Z","iopub.status.idle":"2024-12-27T20:41:22.690287Z","shell.execute_reply.started":"2024-12-27T20:41:22.630715Z","shell.execute_reply":"2024-12-27T20:41:22.689247Z"}}
vis.plot_parallel_coordinate(study, params = ['scaler','look_back','dropout_rate'])

# %% [code]
vis.plot_param_importances(study)


# %% [code] {"execution":{"execution_failed":"2024-12-27T14:25:17.807Z"}}
#pd.reset_option('display.precision')

# %% [code] {"jupyter":{"outputs_hidden":false},"execution":{"execution_failed":"2024-12-27T14:25:17.807Z"}}
endtime = datetime.now()
print(endtime-starttime)